{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for Computer Vision with Python\n",
    "# 10. Neural Network Fundamentals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, N, alpha=0.1):\n",
    "        self.W = np.random.randn(N + 1) / np.sqrt(N)\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def step(self, x):\n",
    "        # apply the step function\n",
    "        return 1 if x > 0 else 0\n",
    "\n",
    "    def fit(self, X, y, epochs=10):\n",
    "        # insert a column of 1’s as the last entry in the feature\n",
    "        # matrix -- this little trick allows us to treat the bias\n",
    "        # as a trainable parameter within the weight matrix\n",
    "        X = np.c_[X, np.ones((X.shape[0]))]\n",
    "        for epoch in np.arange(0, epochs):\n",
    "            # loop over each individual data point\n",
    "            for (x, target) in zip(X, y):\n",
    "                # take the dot product between the input features\n",
    "                # and the weight matrix, then pass this value\n",
    "                # through the step function to obtain the prediction\n",
    "                p = self.step(np.dot(x, self.W))\n",
    "            # only perform a weight update if our prediction\n",
    "            # does not match the target\n",
    "\n",
    "                if p != target:\n",
    "                    # determine the error\n",
    "                    error = p - target\n",
    "                    # update the weight matrix\n",
    "                    self.W += -self.alpha * error * x\n",
    "\n",
    "    def predict(self, X, addBias=True):\n",
    "        # ensure our input is a matrix\n",
    "        X = np.atleast_2d(X)\n",
    "\n",
    "        # check to see if the bias column should be added\n",
    "        if addBias:\n",
    "        # insert a column of 1’s as the last entry in the feature\n",
    "        # matrix (bias)\n",
    "            X = np.c_[X, np.ones((X.shape[0]))]\n",
    "\n",
    "        # take the dot product between the input features and the\n",
    "        # weight matrix, then pass the value through the step\n",
    "        # function\n",
    "        return self.step(np.dot(X, self.W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Evaluating the Perceptron Bitwise Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "perceptron_or.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [1]])\n",
    "print(\"[INFO] training perceptron...\")\n",
    "p = Perceptron(X.shape[1], alpha=0.1)\n",
    "p.fit(X, y, epochs=20)\n",
    "\n",
    "print(\"[INFO] testing perceptron...\")\n",
    "for (x, target) in zip(X, y):\n",
    "    pred = p.predict(x)\n",
    "    print(\"[INFO] data={}, ground-truth={}, pred={}\".format(x, target[0], pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " perceptron_and.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [0], [0], [1]])\n",
    "print(\"[INFO] training perceptron...\")\n",
    "p = Perceptron(X.shape[1], alpha=0.1)\n",
    "p.fit(X, y, epochs=20)\n",
    "\n",
    "print(\"[INFO] testing perceptron...\")\n",
    "for (x, target) in zip(X, y):\n",
    "    pred = p.predict(x)\n",
    "    print(\"[INFO] data={}, ground-truth={}, pred={}\".format(x, target[0], pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "perceptron_xor.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "print(\"[INFO] training perceptron...\")\n",
    "p = Perceptron(X.shape[1], alpha=0.1)\n",
    "p.fit(X, y, epochs=20)\n",
    "\n",
    "print(\"[INFO] testing perceptron...\")\n",
    "for (x, target) in zip(X, y):\n",
    "    pred = p.predict(x)\n",
    "    print(\"[INFO] data={}, ground-truth={}, pred={}\".format(x, target[0], pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, alpha=0.1):\n",
    "        # initialize the list of weights matrices, then store the\n",
    "        # network architecture and learning rate\n",
    "        self.W = []\n",
    "        self.layers = layers\n",
    "        self.alpha = alpha\n",
    "        for i in np.arange(0, len(layers) - 2):\n",
    "            # randomly initialize a weight matrix connecting the\n",
    "            # number of nodes in each respective layer together,\n",
    "            # adding an extra node for the bias\n",
    "\n",
    "            w = np.random.randn(layers[i] + 1, layers[i + 1] + 1)\n",
    "\n",
    "            self.W.append(w / np.sqrt(layers[i]))\n",
    "\n",
    "        w = np.random.randn(layers[-2] + 1, layers[-1])\n",
    "        self.W.append(w / np.sqrt(layers[-2]))\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"NeuralNetwork: {}\".format(\n",
    "            \"-\".join(str(l) for l in self.layers))\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        # compute and return the sigmoid activation value for a\n",
    "        # given input value\n",
    "        return 1.0 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_deriv(self, x):\n",
    "        # compute the derivative of the sigmoid function ASSUMING\n",
    "\n",
    "        # that ‘x‘ has already been passed through the ‘sigmoid‘\n",
    "        # function\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def fit(self, X, y, epochs=1000, displayUpdate=100):\n",
    "        X = np.c_[X, np.ones((X.shape[0]))]\n",
    "        for epoch in np.arange(0, epochs):\n",
    "            for (x, target) in zip(X, y):\n",
    "                 self.fit_partial(x, target)\n",
    "            if epoch == 0 or (epoch + 1) % displayUpdate == 0:\n",
    "                loss = self.calculate_loss(X, y)\n",
    "                print(\"[INFO] epoch={}, loss={:.7f}\".format(\n",
    "                     epoch + 1, loss))\n",
    "\n",
    "    def fit_partial(self, x, y):\n",
    "        A = [np.atleast_2d(x)]\n",
    "        for layer in np.arange(0, len(self.W)):\n",
    "            net = A[layer].dot(self.W[layer])\n",
    "            out = self.sigmoid(net)\n",
    "            A.append(out)\n",
    "        error = A[-1] - y\n",
    "        D = [error * self.sigmoid_deriv(A[-1])]\n",
    "        for layer in np.arange(len(A) - 2, 0, -1):\n",
    "            delta = D[-1].dot(self.W[layer].T)\n",
    "            delta = delta * self.sigmoid_deriv(A[layer])\n",
    "            D.append(delta)\n",
    "        D = D[::-1]\n",
    "        for layer in np.arange(0, len(self.W)):\n",
    "            self.W[layer] += -self.alpha * A[layer].T.dot(D[layer])\n",
    "\n",
    "    def predict(self, X, addBias=True):\n",
    "        p = np.atleast_2d(X)\n",
    "        if addBias:\n",
    "            p = np.c_[p, np.ones((p.shape[0]))]\n",
    "        for layer in np.arange(0, len(self.W)):\n",
    "            p = self.sigmoid(np.dot(p, self.W[layer]))\n",
    "        return p\n",
    "\n",
    "    def calculate_loss(self, X, targets):\n",
    "        targets = np.atleast_2d(targets)\n",
    "        predictions = self.predict(X, addBias=False)\n",
    "        loss = 0.5 * np.sum((predictions - targets) ** 2)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork([2, 2, 1])\n",
    "print(nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn_xor.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "nn = NeuralNetwork([2, 2, 1], alpha=0.5)\n",
    "nn.fit(X, y, epochs=20000)\n",
    "\n",
    "for (x, target) in zip(X, y):\n",
    "    pred = nn.predict(x)[0][0]\n",
    "    step = 1 if pred > 0.5 else 0\n",
    "    print(\"[INFO] data={}, ground-truth={}, pred={:.4f}, step={}\".format( x, target[0], pred, step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn = NeuralNetwork([2, 2, 1], alpha=0.5)  ===>nn = NeuralNetwork([2, 1], alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "nn =  NeuralNetwork([2, 1], alpha=0.5)\n",
    "nn.fit(X, y, epochs=20000)\n",
    "\n",
    "for (x, target) in zip(X, y):\n",
    "    pred = nn.predict(x)[0][0]\n",
    "    step = 1 if pred > 0.5 else 0\n",
    "    print(\"[INFO] data={}, ground-truth={}, pred={:.4f}, step={}\".format(x, target[0], pred, step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation with Python Example: MNIST Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn_mnist.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] loading MNIST (sample) dataset...\")\n",
    "digits = datasets.load_digits()\n",
    "data = digits.data.astype(\"float\")\n",
    "print(data.shape)\n",
    "data = (data - data.min()) / (data.max() - data.min())\n",
    "print(\"[INFO] samples: {}, dim: {}\".format(data.shape[0],data.shape[1]))\n",
    "(trainX, testX, trainY, testY) = train_test_split(data,digits.target, test_size=0.25)\n",
    "trainY = LabelBinarizer().fit_transform(trainY)\n",
    "testY = LabelBinarizer().fit_transform(testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] training network...\")\n",
    "nn = NeuralNetwork([trainX.shape[1], 32, 16, 10])\n",
    "print(\"[INFO] {}\".format(nn))\n",
    "nn.fit(trainX, trainY, epochs=1000)  #教材上是1000，由于时间关系，我们把它改为200\n",
    "\n",
    "print(\"[INFO] evaluating network...\")\n",
    "predictions = nn.predict(testX)\n",
    "predictions = predictions.argmax(axis=1)\n",
    "print(classification_report(testY.argmax(axis=1), predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keras_mnist.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import SGD\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.fetch_mldata(\"MNIST Original\",data_home='C:\\\\Users\\\\86136\\\\scikit_learn_data') #package版本有差异\n",
    "\n",
    "data = dataset.data.astype(\"float\") / 255.0\n",
    "print(data.shape)\n",
    "(trainX, testX, trainY, testY) = train_test_split(data,dataset.target, test_size = 0.25)\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "trainY = lb.fit_transform(trainY)\n",
    "testY = lb.transform(testY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(256, input_shape=(784,), activation=\"sigmoid\"))\n",
    "model.add(Dense(128, activation=\"sigmoid\"))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "print(\"[INFO] training network...\")\n",
    "\n",
    "\n",
    "sgd = SGD(0.01)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=sgd,metrics = [\"accuracy\"])\n",
    "H = model.fit(trainX, trainY, validation_data=(testX, testY),epochs = 100, batch_size = 128)\n",
    "\n",
    "print(\"[INFO] evaluating network...\")\n",
    "predictions = model.predict(testX, batch_size=128)\n",
    "print(classification_report(testY.argmax(axis=1),predictions.argmax(axis=1),\n",
    "        target_names = [str(x) for x in lb.classes_]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, 100), H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, 100), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "# plt.plot(np.arange(0, 100), H.history[\"acc\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0, 100), H.history[\"accuracy\"], label=\"train_acc\")\n",
    "# plt.plot(np.arange(0, 100), H.history[\"val_acc\"], label=\"val_acc\")\n",
    "plt.plot(np.arange(0, 100), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend()\n",
    "# plt.savefig(args[\"output\"])\n",
    "plt.show()\n",
    "# plt.savefig(\"./output.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets.base import  get_data_home\n",
    "print(get_data_home())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keras_cifar10.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] loading CIFAR-10 data...\")\n",
    "((trainX, trainY), (testX, testY)) = cifar10.load_data()\n",
    "trainX = trainX.astype(\"float\") / 255.0\n",
    "testX = testX.astype(\"float\") / 255.0\n",
    "trainX = trainX.reshape((trainX.shape[0], 3072))\n",
    "testX = testX.reshape((testX.shape[0], 3072))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = LabelBinarizer()\n",
    "trainY = lb.fit_transform(trainY)\n",
    "testY = lb.transform(testY)\n",
    "# initialize the label names for the CIFAR-10 dataset\n",
    "labelNames = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\",\n",
    "                  \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1024, input_shape=(3072,), activation=\"relu\"))\n",
    "model.add(Dense(512, activation=\"relu\"))\n",
    "model.add(Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] training network...\")\n",
    "sgd = SGD(0.01)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=sgd,\n",
    "        metrics = [\"accuracy\"])\n",
    "# H = model.fit(trainX, trainY, validation_data=(testX, testY),epochs = 100, batch_size = 32)\n",
    "H = model.fit(trainX, trainY, validation_data=(testX, testY),epochs = 1, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] evaluating network...\")\n",
    "predictions = model.predict(testX, batch_size=32)\n",
    "print(classification_report(testY.argmax(axis=1),\n",
    "predictions.argmax(axis=1), target_names = labelNames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, 100), H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, 100), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0, 100), H.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0, 100), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend()\n",
    "# plt.savefig('./output')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mxnet",
   "language": "python",
   "name": "mxnet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
